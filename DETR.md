[[Transformer]]
[[ViT]]
##### Intro
ViT解决了Transformer如何应用到CV领域最基础的任务——图像分类的问题。

接下来很自然的问题：如何解决目标检测问题？

DETR（Detection Transformer）是在​​2020年​​提出的，其原论文标题为 ​​《End-to-End Object Detection with Transformers》

首次将Transformer的编码器-解码器结构完整引入目标检测任务，通过​​全局注意力机制​​替代了CNN的局部感受野设计，并利用​​二分图匹配（匈牙利算法）​​ 实现预测框与真实框的一一对应，从而彻底消除了传统检测器中必需的NMS后处理步骤
##### 流程简述
1. Resnet50特征提取：(3×800×1066) -> ​(2048×25×34)
2. 1x1卷积降维：​(2048×25×34) -> (256x25x34)
3. 空间展平（至此完成嵌入）：(256x25x34) -> (850x256)
4. 叠加二维正弦位置编码：维度不变
5. Transformer多头自注意力编码：维度不变
6. Transformer多头交叉注意力解码：(850x256)->(100x256)
7. 预测头
	1. 分类预测头，Linear：(100x256)->(100xC)，C为包含背景类的类别数量
	2. 坐标预测头，Linear：(100x256)->(100x4)
##### 关键结论
1. 在解码过程中，预设一组包含100个维度256的可学习向量的作为Queries。每个Query负责预测​**​一个目标​**​的边界框。也就是说，要求模型从每一张图片中检出100个目标。100个Query最终生成100个加权平均后的V向量。
2. 计算损失时，将一张图片中的真值框信息，用void填充到100个，在数量上与模型输出对齐。然后使用匈牙利匹配进行真值框与检出框的配对，然后针对匹配成功的pair即可计算分类损失和交点坐标回归损失。
3. 局限性：
	1. 小目标检测差（因为没有设计类似FPN的多尺度）
	2. 收敛慢（因为随机初始化的Queries缺乏位置先验）



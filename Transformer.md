[[RNN]]
##### Intro
于2017年NIPS上被提出，原论文“Attention Is All You Need”，用于翻译任务。
##### 流程简述
- 编码：嵌入->叠加位置编码->n层多头自注意力单元

- 解码：嵌入->叠加位置编码->n层多头交叉注意力单元->Linear->Softmax

- 多头自注意力单元：多头自注意力（QKV映射、加权平均、多头拼接）、短路叠加、层归一化、MLP、短路叠加、层归一化

- 多头交叉注意力单元：多头自注意力、短路叠加、层归一化、多头交叉注意力、短路叠加、层归一化、MLP、短路叠加、层归一化
##### 关键结论
- 编码不改变向量维度。这意味着位置编码、自注意力计算都未改变嵌入向量的维度，只是在其中增加了更多信息。这也是为什么可以进行短路叠加

- 编码向量在N个解码器注意力单元中重复作为K-V使用

- 并行性的体现：
	- 单个样本内，多个词并行处理
		- 编码时: 一个样本包含多个词。一个词对应一个input向量。各个input向量彼此独立，可以并行编码;
		- 解码时：
			- 训练时：使用mask遮盖当前解码位置后方的词。其实是空间换时间，将一句包含n个词的样本，视作n个样本处理。
			- 推理时：无法并行，必须进行串行自回归解码
	- 多个样本，进行批处理

##### 概念解释
- 编码：目的是将一句话中的一个词，转化为一个列向量。其中包含
	- 这个词的语义信息
	- 这个词的位置信息
	- 这个词与其他词的相互作用信息

- 解码：目的是将一个列向量转换回一个自然语言的词

- 嵌入：词的向量化表达
	- 经典方法1：one-hot
	- 经典方法2：单个数字映射
	- 主流方法：Embedding，介于one-hot和每个词用一个数字表示，这两种极端方法之间的一种词的向量化表达方式。已知词表长度为n，再设定一个维度参数k，用一个k维向量来表示这个词。嵌入矩阵一般是一个可学习矩阵。

- 位置编码：将一个词在一句话中的位置，编码到这个词对应的向量化表示中
	- 方式1：使用一个显式的函数进行规则化的映射，如原论文中的正弦编码
	- 方式2：将其作为一个可学习的参数，通过训练得到

- 自注意力：Q向量和K-V向量来源相同，是同一个向量经过QKV矩阵映射得到

- 交叉注意力：Q向量和K-V向量来源不相同

- 多头注意力：用n组QKV矩阵，将一个维度为k的待进行注意力操作的列向量以k/n为维度，映射为n组QKV进行注意力计算，将最终结果再拼接到一起得到最终的k维向量
